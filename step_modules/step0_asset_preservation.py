"""
Step 0 Module - ã‚¢ã‚»ãƒƒãƒˆæƒ…å ±ä¿å­˜
ç‹¬ç«‹ã—ãŸå®Ÿè¡Œæ©Ÿèƒ½ã¨ã—ã¦ã€3Dãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è©³ç´°ãªã‚¢ã‚»ãƒƒãƒˆæƒ…å ±ï¼ˆUVã€ãƒãƒ†ãƒªã‚¢ãƒ«ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ç­‰ï¼‰ã‚’ä¿å­˜

è²¬å‹™: ã‚ªãƒªã‚¸ãƒŠãƒ«3Dãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« â†’ ã‚¢ã‚»ãƒƒãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON + ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
å…¥åŠ›: 3Dãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.glb, .fbx, .objç­‰)
å‡ºåŠ›: 
    - ã‚¢ã‚»ãƒƒãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    - ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚¯ã‚¹ãƒãƒ£ãŒæ ¼ç´ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
"""
import os
import sys
import json
import time
import logging
import shutil
from pathlib import Path
from typing import Tuple, Dict, Any

# UniRigå®Ÿè¡Œãƒ‘ã‚¹è¨­å®š (å¿…è¦ã«å¿œã˜ã¦)
# sys.path.append(str(Path(__file__).resolve().parents[1])) # /app ã‚’æŒ‡ã™ã‚ˆã†ã«èª¿æ•´

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class Step0AssetPreservation:
    """Step 0: ã‚¢ã‚»ãƒƒãƒˆæƒ…å ±ä¿å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""

    def __init__(self, output_dir: Path):
        self.base_output_dir = Path(output_dir) # e.g., /app/pipeline_work/00_asset_preservation
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Step0AssetPreservation initialized. Base output directory: {self.base_output_dir}")

    def preserve_assets(self, input_file: str, model_name: str) -> Tuple[bool, str, Dict[str, Any]]:
        """
        ã‚¢ã‚»ãƒƒãƒˆè©³ç´°ï¼ˆUVã€ãƒãƒ†ãƒªã‚¢ãƒ«ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰ã‚’ä¿å­˜ã—ã¾ã™ã€‚
        Blender (bpy) ã‚’ä½¿ç”¨ã—ãŸå‡¦ç†ã‚’ã“ã“ã«è¿½åŠ ã—ã¾ã™ã€‚

        Args:
            input_file: ã‚ªãƒªã‚¸ãƒŠãƒ«3Dãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
            model_name: ãƒ¢ãƒ‡ãƒ«ã®ä¸€æ„ãªåå‰ã¾ãŸã¯è­˜åˆ¥å­

        Returns:
            success: ä¿å­˜ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€ãã‚Œä»¥å¤–ã¯False
            logs: ä¿å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€æ–‡å­—åˆ—
            output_files: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã‚’å«ã‚€è¾æ›¸
        """
        logs = f"Step 0: Asset Preservation for {model_name} from {input_file}\\n"
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š (e.g., /app/pipeline_work/00_asset_preservation/bird_test)
        model_specific_output_dir = self.base_output_dir / model_name
        try:
            model_specific_output_dir.mkdir(parents=True, exist_ok=True)
            logs += f"Output directory for {model_name}: {model_specific_output_dir}\\n"
        except Exception as e:
            error_msg = f"Error creating output directory {model_specific_output_dir}: {e}"
            logger.error(error_msg)
            return False, logs + error_msg, {}

        # ãƒ†ã‚¯ã‚¹ãƒãƒ£ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        preserved_textures_dir = model_specific_output_dir / "textures"
        try:
            preserved_textures_dir.mkdir(exist_ok=True)
            logs += f"Textures directory: {preserved_textures_dir}\\n"
        except Exception as e:
            error_msg = f"Error creating textures directory {preserved_textures_dir}: {e}"
            logger.error(error_msg)
            return False, logs + error_msg, {}

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        asset_metadata_file = model_specific_output_dir / f"{model_name}_asset_metadata.json"
        logs += f"Asset metadata JSON path: {asset_metadata_file}\\n"

        output_files: Dict[str, Any] = {
            "asset_metadata_json": str(asset_metadata_file),
            "preserved_textures_dir": str(preserved_textures_dir),
            "model_name": model_name,
            "input_file": input_file
        }

        try:
            logger.info(f"Starting asset preservation for {model_name} from {input_file}")

            if not Path(input_file).exists():
                error_msg = f"Input file not found: {input_file}"
                logger.error(error_msg)
                return False, logs + error_msg, output_files
            
            metadata = {
                "model_name": model_name,
                "original_file_path": input_file,
                "preservation_timestamp": time.time(),
                "preserved_textures_relative_dir": "textures", # textures_dirã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
                "uv_maps": [], # Placeholder for UV map data
                "materials": [], # Placeholder for material data
                "textures": [], # Placeholder for texture file list and info
                "blender_version": None # Placeholder for Blender version used
            }
            
            # æ‹¡å¼µãƒ†ã‚¯ã‚¹ãƒãƒ£æŠ½å‡ºã‚’å®Ÿè¡Œ
            logs += "Starting enhanced texture extraction with Blender...\\n"
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®åŸºæœ¬æƒ…å ±ã‚’è¨˜éŒ²
            input_path = Path(input_file)
            file_stats = input_path.stat()
            
            # Blenderæ‹¡å¼µãƒ†ã‚¯ã‚¹ãƒãƒ£æŠ½å‡ºã‚’å®Ÿè¡Œ
            success, extraction_logs, enhanced_metadata = self._extract_textures_with_blender(
                input_file, model_name, preserved_textures_dir
            )
            
            logs += extraction_logs
            
            if success and enhanced_metadata:
                # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åŸºæœ¬æƒ…å ±ã¨çµåˆ
                metadata = {
                    "model_name": model_name,
                    "original_file_path": input_file,
                    "original_file_size": file_stats.st_size,
                    "original_file_extension": input_path.suffix,
                    "preservation_timestamp": time.time(),
                    "preserved_textures_relative_dir": "textures",
                    "status": "enhanced_preserved",
                    **enhanced_metadata  # UVã€ãƒãƒ†ãƒªã‚¢ãƒ«ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£æƒ…å ±ã‚’çµ±åˆ
                }
                logs += "âœ… Enhanced texture extraction completed successfully\\n"
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿
                metadata = {
                    "model_name": model_name,
                    "original_file_path": input_file,
                    "original_file_size": file_stats.st_size,
                    "original_file_extension": input_path.suffix,
                    "preservation_timestamp": time.time(),
                    "preserved_textures_relative_dir": "textures",
                    "status": "basic_preserved",
                    "uv_maps": [],
                    "materials": [],
                    "textures": [],
                    "blender_version": None
                }
                logs += "âš ï¸ Fallback to basic preservation due to extraction issues\\n"
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã¿
            with open(asset_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logs += f"Asset metadata saved to: {asset_metadata_file}\\n"
            logs += "Asset preservation completed successfully\\n"
            
            logger.info(f"Asset preservation completed for {model_name}")
            return True, logs, output_files

        except Exception as e:
            error_msg = f"Error during asset preservation for {model_name}: {e}"
            logger.error(error_msg, exc_info=True)
            return False, logs + error_msg, output_files

    def _extract_textures_with_blender(self, input_file: str, model_name: str, textures_dir: Path) -> Tuple[bool, str, Dict]:
        """
        Blenderã‚’ä½¿ç”¨ã—ã¦æ‹¡å¼µãƒ†ã‚¯ã‚¹ãƒãƒ£æŠ½å‡ºã‚’å®Ÿè¡Œ
        
        Args:
            input_file: å…¥åŠ›3Dãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            model_name: ãƒ¢ãƒ‡ãƒ«å
            textures_dir: ãƒ†ã‚¯ã‚¹ãƒãƒ£ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            (success, logs, enhanced_metadata)
        """
        try:
            # Blenderã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            import bpy
            
            logs = ""
            
            # Blenderã‚·ãƒ¼ãƒ³ã‚¯ãƒªã‚¢
            bpy.ops.object.select_all(action='SELECT')
            bpy.ops.object.delete()
            
            for mesh in bpy.data.meshes:
                bpy.data.meshes.remove(mesh)
            for material in bpy.data.materials:
                bpy.data.materials.remove(material)
            for texture in bpy.data.textures:
                bpy.data.textures.remove(texture)
            for image in bpy.data.images:
                bpy.data.images.remove(image)
            
            logs += "âœ… Blenderã‚·ãƒ¼ãƒ³ã‚¯ãƒªã‚¢å®Œäº†\\n"
            
            # 3Dãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            file_ext = Path(input_file).suffix.lower()
            if file_ext == '.glb' or file_ext == '.gltf':
                bpy.ops.import_scene.gltf(filepath=input_file)
                logs += f"âœ… GLB/GLTFãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {input_file}\\n"
            elif file_ext == '.fbx':
                bpy.ops.import_scene.fbx(filepath=input_file)
                logs += f"âœ… FBXãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {input_file}\\n"
            elif file_ext == '.obj':
                bpy.ops.import_scene.obj(filepath=input_file)
                logs += f"âœ… OBJãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {input_file}\\n"
            else:
                return False, f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_ext}\\n", {}
            
            # UVã€ãƒãƒ†ãƒªã‚¢ãƒ«ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£æƒ…å ±æŠ½å‡º
            enhanced_metadata = {
                "uv_maps": [],
                "materials": [],
                "textures": [],
                "blender_version": bpy.app.version_string
            }
            
            texture_count = 0
            
            # å…¨ãƒ¡ãƒƒã‚·ãƒ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‡¦ç†
            for obj in bpy.data.objects:
                if obj.type != 'MESH':
                    continue
                
                mesh = obj.data
                logs += f"ğŸ“ ãƒ¡ãƒƒã‚·ãƒ¥å‡¦ç†ä¸­: {obj.name} (é ‚ç‚¹æ•°: {len(mesh.vertices):,})\\n"
                
                # UVæƒ…å ±æŠ½å‡º
                if mesh.uv_layers:
                    for uv_layer in mesh.uv_layers:
                        uv_data = []
                        for loop in mesh.loops:
                            uv = uv_layer.data[loop.index].uv
                            uv_data.append([float(uv.x), float(uv.y)])
                        
                        enhanced_metadata["uv_maps"].append({
                            "name": uv_layer.name,
                            "object_name": obj.name,
                            "uv_count": len(uv_data),
                            "uv_coordinates": uv_data[:100] if len(uv_data) > 100 else uv_data  # æœ€åˆã®100å€‹ã®ã¿ä¿å­˜
                        })
                        logs += f"ğŸ“ UV Layer: {uv_layer.name} ({len(uv_data):,} coordinates)\\n"
                
                # ãƒãƒ†ãƒªã‚¢ãƒ«å‡¦ç†
                for mat_slot in obj.material_slots:
                    if not mat_slot.material:
                        continue
                    
                    material = mat_slot.material
                    
                    # ãƒãƒ†ãƒªã‚¢ãƒ«åŸºæœ¬æƒ…å ±
                    mat_info = {
                        "name": material.name,
                        "object_name": obj.name,
                        "use_nodes": material.use_nodes,
                        "node_tree": None
                    }
                    
                    # ãƒãƒ†ãƒªã‚¢ãƒ«ãƒãƒ¼ãƒ‰å‡¦ç†
                    if material.use_nodes and material.node_tree:
                        node_tree_data = {
                            "nodes": {},
                            "links": []
                        }
                        
                        # ãƒãƒ¼ãƒ‰æƒ…å ±æŠ½å‡º
                        for node in material.node_tree.nodes:
                            node_data = {
                                "type": node.type,
                                "bl_idname": node.bl_idname,
                                "location": [float(node.location.x), float(node.location.y)],
                                "inputs": [],
                                "outputs": []
                            }
                            
                            # å…¥åŠ›æƒ…å ±
                            for input_socket in node.inputs:
                                input_info = {
                                    "name": input_socket.name,
                                    "type": input_socket.type,
                                    "default_value": None
                                }
                                
                                try:
                                    if hasattr(input_socket, 'default_value'):
                                        value = input_socket.default_value
                                        # JSON serializable å½¢å¼ã«å¤‰æ›
                                        if hasattr(value, '__iter__') and not isinstance(value, str):
                                            try:
                                                input_info["default_value"] = list(value)
                                            except:
                                                input_info["default_value"] = str(value)
                                        elif hasattr(value, '__dict__'):
                                            # Blenderã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆEulerç­‰ï¼‰ã®å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
                                            input_info["default_value"] = str(value)
                                        else:
                                            # ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–å‹ã®å ´åˆ
                                            try:
                                                # JSON serializable ã‹ãƒ†ã‚¹ãƒˆ
                                                import json
                                                json.dumps(value)
                                                input_info["default_value"] = value
                                            except (TypeError, ValueError):
                                                input_info["default_value"] = str(value)
                                except Exception as e:
                                    input_info["default_value"] = f"extraction_error: {e}"
                                
                                node_data["inputs"].append(input_info)
                            
                            # å‡ºåŠ›æƒ…å ±
                            for output_socket in node.outputs:
                                node_data["outputs"].append({
                                    "name": output_socket.name,
                                    "type": output_socket.type
                                })
                            
                            # ãƒ†ã‚¯ã‚¹ãƒãƒ£ç”»åƒãƒãƒ¼ãƒ‰å‡¦ç†
                            if node.type == 'TEX_IMAGE' and node.image:
                                image = node.image
                                
                                # ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆGLBåŸ‹ã‚è¾¼ã¿ãƒ†ã‚¯ã‚¹ãƒãƒ£ã«ã‚‚å¯¾å¿œï¼‰
                                try:
                                    # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                                    original_name = Path(image.name).stem
                                    file_ext = Path(image.name).suffix or '.png'
                                    texture_filename = f"{model_name}_{original_name}_{texture_count:03d}{file_ext}"
                                    texture_path = textures_dir / texture_filename
                                    
                                    # ãƒ†ã‚¯ã‚¹ãƒãƒ£ä¿å­˜å®Ÿè¡Œï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ»å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«å•ã‚ãšï¼‰
                                    image.save_render(str(texture_path))
                                    texture_count += 1
                                    
                                    # ãƒ†ã‚¯ã‚¹ãƒãƒ£æƒ…å ±è¨˜éŒ²
                                    texture_info = {
                                        "texture_file_path": texture_filename,
                                        "original_name": image.name,
                                        "width": image.size[0],
                                        "height": image.size[1],
                                        "format": image.file_format,
                                        "source": image.source,
                                        "filepath": image.filepath if hasattr(image, 'filepath') else "",
                                        "node_name": node.name,
                                        "material_name": material.name,
                                        "object_name": obj.name
                                    }
                                    enhanced_metadata["textures"].append(texture_info)
                                    
                                    # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã«ãƒ†ã‚¯ã‚¹ãƒãƒ£æƒ…å ±è¿½åŠ 
                                    node_data["image_info"] = texture_info
                                    
                                    logs += f"ğŸ¨ ãƒ†ã‚¯ã‚¹ãƒãƒ£ä¿å­˜: {texture_filename} ({image.size[0]}x{image.size[1]}) [source: {image.source}]\\n"
                                except Exception as e:
                                    logs += f"âš ï¸ ãƒ†ã‚¯ã‚¹ãƒãƒ£ä¿å­˜å¤±æ•—: {image.name} - {e}\\n"
                            
                            node_tree_data["nodes"][node.name] = node_data
                        
                        # ãƒªãƒ³ã‚¯æƒ…å ±æŠ½å‡º
                        for link in material.node_tree.links:
                            link_data = {
                                "from_node": link.from_node.name,
                                "from_socket": link.from_socket.name,
                                "to_node": link.to_node.name,
                                "to_socket": link.to_socket.name
                            }
                            node_tree_data["links"].append(link_data)
                        
                        mat_info["node_tree"] = node_tree_data
                    
                    enhanced_metadata["materials"].append(mat_info)
                    logs += f"ğŸ¨ ãƒãƒ†ãƒªã‚¢ãƒ«å‡¦ç†: {material.name}\\n"
            
            logs += f"âœ… æ‹¡å¼µãƒ†ã‚¯ã‚¹ãƒãƒ£æŠ½å‡ºå®Œäº†: {texture_count}å€‹ã®ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚’æŠ½å‡º\\n"
            logs += f"ğŸ“ UV Maps: {len(enhanced_metadata['uv_maps'])}å€‹\\n"
            logs += f"ğŸ¨ Materials: {len(enhanced_metadata['materials'])}å€‹\\n"
            logs += f"ğŸ–¼ï¸ Textures: {len(enhanced_metadata['textures'])}å€‹\\n"
            
            return True, logs, enhanced_metadata
            
        except ImportError:
            return False, "âŒ Blenderãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“\\n", {}
        except Exception as e:
            error_logs = f"âŒ Blenderæ‹¡å¼µãƒ†ã‚¯ã‚¹ãƒãƒ£æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}\\n"
            logger.error(error_logs, exc_info=True)
            return False, error_logs, {}


def execute_step0(input_file: str, model_name: str, output_dir: str) -> Tuple[bool, str, Dict[str, Any]]:
    """
    Step 0: ã‚¢ã‚»ãƒƒãƒˆæƒ…å ±ä¿å­˜ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Args:
        input_file: 3Dãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model_name: ãƒ¢ãƒ‡ãƒ«å
        output_dir: ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ä¾‹: /app/pipeline_work/00_asset_preservation)

    Returns:
        success: æˆåŠŸãƒ•ãƒ©ã‚° (True/False)
        logs: å®Ÿè¡Œãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        output_files: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¾æ›¸
    """
    try:
        preserver = Step0AssetPreservation(output_dir=Path(output_dir))
        return preserver.preserve_assets(input_file, model_name)
    except Exception as e:
        error_msg = f"Step 0 execution failed: {e}"
        logger.error(error_msg, exc_info=True)
        return False, error_msg, {}


if __name__ == "__main__":
    print("--- Running Step 0: Asset Preservation Test ---")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®è¨­å®š
    # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚„ãƒ¢ãƒ‡ãƒ«åã«ç½®ãæ›ãˆã¦ãã ã•ã„
    # Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ‘ã‚¹ã‚’æƒ³å®š
    test_input_file_original = "/app/examples/bird.glb" 
    test_model_name = "bird_step0_test"
    
    # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«åˆã‚ã›ã‚‹
    # ä¾‹: /app/pipeline_work/00_asset_preservation/
    test_base_output_dir = Path("/app/pipeline_work_test/00_asset_preservation")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‘ã‚¹
    # model_specific_output_dir_for_cleanup = test_base_output_dir / test_model_name

    # loggerã®è¨­å®šã‚’ãƒ†ã‚¹ãƒˆç”¨ã«å¤‰æ›´ (ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãªã©)
    # test_log_file = Path("/app/logs") / "step0_test.log"
    # test_log_file.parent.mkdir(exist_ok=True)
    # file_handler = logging.FileHandler(test_log_file)
    # file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    # logger.addHandler(file_handler)
    # logger.propagate = False # Prevent double logging to console if root logger also has console handler

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ (å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’ä½œæˆ)
    test_input_file = Path(test_input_file_original)
    created_dummy_input = False
    if not test_input_file.exists():
        print(f"Test input file not found: {test_input_file}. Creating a dummy file.")
        try:
            test_input_file.parent.mkdir(parents=True, exist_ok=True)
            with open(test_input_file, 'w') as f:
                f.write("dummy glb data for testing Step 0")
            created_dummy_input = True
            print(f"Dummy file created: {test_input_file}")
        except Exception as e:
            print(f"Could not create dummy input file {test_input_file}: {e}")
            sys.exit(1) # Exit if dummy cannot be created

    print(f"Executing: execute_step0(input_file='{str(test_input_file)}', model_name='{test_model_name}', output_dir=Path('{str(test_base_output_dir)}'))")
    
    success, logs, files = execute_step0(str(test_input_file), test_model_name, test_base_output_dir)
    
    print("\\n--- Test Execution Result ---")
    print(f"  Success: {success}")
    print(f"  Logs:\\n{logs}")
    print(f"  Output Files: {json.dumps(files, indent=2)}")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
    print("\\n--- Cleanup ---")
    # model_specific_output_dir_to_delete = test_base_output_dir / test_model_name
    # if model_specific_output_dir_to_delete.exists():
    #     try:
    #         shutil.rmtree(model_specific_output_dir_to_delete)
    #         print(f"  Cleaned up: {model_specific_output_dir_to_delete}")
    #     except Exception as e:
    #         print(f"  Error during cleanup of {model_specific_output_dir_to_delete}: {e}")
    # else:
    #     print(f"  Cleanup: Directory not found, no need to delete {model_specific_output_dir_to_delete}")

    if created_dummy_input and test_input_file.exists():
        try:
            os.remove(test_input_file)
            print(f"  Cleaned up dummy input file: {test_input_file}")
        except Exception as e:
            print(f"  Error cleaning up dummy input file {test_input_file}: {e}")
            
    # # Remove test base output dir if empty, or if it only contains the model_name dir which was removed
    # try:
    #     if test_base_output_dir.exists() and not any(test_base_output_dir.iterdir()):
    #         test_base_output_dir.rmdir()
    #         print(f"  Cleaned up base test directory: {test_base_output_dir}")
    # except Exception as e:
    #     print(f"  Error cleaning up base test directory {test_base_output_dir}: {e}")
        
    print("--- Test Run Complete ---")

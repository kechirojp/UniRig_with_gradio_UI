"""
Step 2 Module - ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆ
ç‹¬ç«‹ã—ãŸå®Ÿè¡Œæ©Ÿèƒ½ã¨ã—ã¦ã€AIã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ«ãƒˆãƒ³æ§‹é€ ã‚’ç”Ÿæˆ

è²¬å‹™: ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ â†’ ã‚¹ã‚±ãƒ«ãƒˆãƒ³FBX + ã‚¹ã‚±ãƒ«ãƒˆãƒ³ãƒ‡ãƒ¼ã‚¿
å…¥åŠ›: ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npz), æ€§åˆ¥è¨­å®š
å‡ºåŠ›: ã‚¹ã‚±ãƒ«ãƒˆãƒ³FBXãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹, ã‚¹ã‚±ãƒ«ãƒˆãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npz)
"""

import os
import sys
import logging
import subprocess
import time
from pathlib import Path
from typing import Tuple, Dict, Optional, Any
import json # Not strictly used, but good for potential metadata
import numpy as np
import shutil

# UniRigãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¸ã®ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/src')

# Default logger setup if no logger is provided
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class Step2Skeleton:
    """Step 2: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
    
    def __init__(self, output_dir: Path, logger_instance: Optional[logging.Logger] = None):
        self.output_dir = output_dir # This is the step-specific output dir, e.g., /app/pipeline_work/model_name/02_skeleton_generated/
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logger_instance if logger_instance else logging.getLogger(__name__)
        
        # UniRigã®ã‚³ã‚¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç‰¹å®šã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’æœŸå¾…ã™ã‚‹å ´åˆãŒã‚ã‚‹
        # ä¾‹: dataset_inference_clean/{model_name}/raw_data.npz
        # ã“ã®ãƒ‘ã‚¹ã¯UniRigã®run.pyã‚„é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å†…éƒ¨å®Ÿè£…ã«ä¾å­˜ã™ã‚‹
        self.unirig_processing_base_dir = Path("/app/dataset_inference_clean") 
        self.unirig_processing_base_dir.mkdir(parents=True, exist_ok=True)
        
    def generate_skeleton(self, 
                          input_npz_path: Path, 
                          model_name: str, 
                          original_model_file_for_fbx_copy: Path, 
                          gender: str = "neutral"
                         ) -> Tuple[bool, str, Dict[str, Any]]:
        """
        ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã®å®Ÿè¡Œã€‚
        UniRigã®run.pyã‚’å‘¼ã³å‡ºã—ã¦ predict_skeleton.npz ã‚’ç”Ÿæˆã—ã€
        å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (FBX/GLB) ã‚’ {model_name}.fbx ã¨ã—ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
        
        Args:
            input_npz_path: å…¥åŠ›ãƒ¡ãƒƒã‚·ãƒ¥NPZãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: Step1å‡ºåŠ›ã® raw_data.npz ã®çµ¶å¯¾ãƒ‘ã‚¹)
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚„UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã«ä½¿ç”¨ï¼‰
            original_model_file_for_fbx_copy: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æŒãŸãªã„ãƒ™ãƒ¼ã‚¹FBXã¨ã—ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                                              (ä¾‹: Step1ãŒä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã—ãŸå…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹)
            gender: æ€§åˆ¥è¨­å®š ("male", "female", "neutral") - ç¾åœ¨UniRigã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ç›´æ¥ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ã‚ã‚Š
            
        Returns:
            (success, logs, output_files dict)
        """
        logs = ""
        try:
            start_time = time.time()
            self.logger.info(f"Step 2 é–‹å§‹: å…¥åŠ›NPZ '{input_npz_path}', ãƒ¢ãƒ‡ãƒ«å '{model_name}', æ€§åˆ¥ '{gender}'")
            self.logger.info(f"FBXã‚³ãƒ”ãƒ¼å…ƒ: '{original_model_file_for_fbx_copy}'")
            
            if not input_npz_path.exists():
                error_msg = f"âŒ å…¥åŠ›ãƒ¡ãƒƒã‚·ãƒ¥NPZãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_npz_path}"
                self.logger.error(error_msg)
                return False, error_msg, {}
            
            if not original_model_file_for_fbx_copy.exists():
                error_msg = f"âŒ FBXã‚³ãƒ”ãƒ¼å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {original_model_file_for_fbx_copy}"
                self.logger.error(error_msg)
                return False, error_msg, {}

            # --- UniRigå‡¦ç†ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ ---
            # UniRigã®run.pyã¯ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€  (dataset_inference_clean/{model_name}/) ã‚’æœŸå¾…ã™ã‚‹
            unirig_model_processing_dir = self.unirig_processing_base_dir / model_name
            unirig_model_processing_dir.mkdir(parents=True, exist_ok=True)
            logs += f"âš™ï¸ UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: '{unirig_model_processing_dir}'\\n"

            # Step1ã‹ã‚‰å—ã‘å–ã£ãŸ raw_data.npz ã‚’ UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            unirig_input_npz_target = unirig_model_processing_dir / "raw_data.npz"
            shutil.copy2(input_npz_path, unirig_input_npz_target)
            logs += f"ğŸ“‹ å…¥åŠ›NPZ '{input_npz_path.name}' ã‚’UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼: '{unirig_input_npz_target}'\\n"

            # --- UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ ---
            success_script, script_logs = self._run_unirig_skeleton_script(
                model_name, unirig_model_processing_dir
            )
            logs += script_logs
            
            if not success_script:
                error_msg = f"âŒ UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå¤±æ•—ã€‚"
                self.logger.error(error_msg + " è©³ç´°ã¯ãƒ­ã‚°å‚ç…§ã€‚")
                return False, logs, {}
            
            # --- ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨æ•´ç† ---
            # UniRigã¯ unirig_model_processing_dir ã« predict_skeleton.npz ã‚’å‡ºåŠ›ã™ã‚‹ã¯ãš
            generated_npz_in_unirig_dir = unirig_model_processing_dir / "predict_skeleton.npz"
            
            if not generated_npz_in_unirig_dir.exists():
                error_msg = f"âŒ UniRigãŒæœŸå¾…ã•ã‚Œã‚‹NPZãƒ•ã‚¡ã‚¤ãƒ« '{generated_npz_in_unirig_dir}' ã‚’ç”Ÿæˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                self.logger.error(error_msg)
                self._debug_list_directory_contents(unirig_model_processing_dir) # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                return False, logs + error_msg + "\\n", {}
            
            logs += f"âœ… UniRigãŒNPZãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ: '{generated_npz_in_unirig_dir}'\\n"

            # ç”Ÿæˆã•ã‚ŒãŸ predict_skeleton.npz ã‚’ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            final_output_npz = self.output_dir / "predict_skeleton.npz" # å›ºå®šå
            shutil.copy2(generated_npz_in_unirig_dir, final_output_npz)
            logs += f"ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸNPZã‚’Step2å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼: '{final_output_npz}'\\n"

            # original_model_file_for_fbx_copy ã‚’ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« {model_name}.fbx ã¨ã—ã¦ã‚³ãƒ”ãƒ¼
            final_output_fbx = self.output_dir / f"{model_name}.fbx" # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—
            shutil.copy2(original_model_file_for_fbx_copy, final_output_fbx)
            logs += f"ğŸ“‹ FBXã‚³ãƒ”ãƒ¼å…ƒ '{original_model_file_for_fbx_copy.name}' ã‚’Step2å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼: '{final_output_fbx}'\\n"
            
            # ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ (final_output_npz ã‹ã‚‰)
            bones_txt_path_str = self._generate_bones_txt_from_npz(final_output_npz, model_name)
            if bones_txt_path_str:
                logs += f"ğŸ“„ ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: '{bones_txt_path_str}'\\n"
            else:
                logs += f"âš ï¸ ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã€‚\\n"

            processing_time = time.time() - start_time
            
            output_files: Dict[str, Any] = {
                "skeleton_fbx": str(final_output_fbx),
                "skeleton_npz": str(final_output_npz),
                "bones_txt": bones_txt_path_str,
                "bone_count": self._count_bones_in_npz_file(final_output_npz),
                "file_size_fbx": final_output_fbx.stat().st_size if final_output_fbx.exists() else 0,
                "file_size_npz": final_output_npz.stat().st_size if final_output_npz.exists() else 0,
                "processing_time_seconds": round(processing_time, 2)
            }
            
            final_log_message = f"Step 2 (ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆ) å®Œäº†:\n"
            final_log_message += f"- å…¥åŠ›NPZ: {input_npz_path.name}\n"
            final_log_message += f"- FBXã‚³ãƒ”ãƒ¼å…ƒ: {original_model_file_for_fbx_copy.name}\n"
            final_log_message += f"- å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’\n"
            final_log_message += f"- å‡ºåŠ›FBX: {output_files['skeleton_fbx']} ({output_files['file_size_fbx']:,} bytes)\n"
            final_log_message += f"- å‡ºåŠ›NPZ: {output_files['skeleton_npz']} ({output_files['file_size_npz']:,} bytes)\n"
            final_log_message += f"- ãƒœãƒ¼ãƒ³æ•° (æ¨å®š): {output_files['bone_count']}\n"
            if output_files['bones_txt']:
                final_log_message += f"- ãƒœãƒ¼ãƒ³éšå±¤ãƒ•ã‚¡ã‚¤ãƒ«: {output_files['bones_txt']}\n"
            logs += "\\n" + final_log_message
            
            self.logger.info(f"Step 2 æ­£å¸¸å®Œäº†ã€‚å‡ºåŠ›FBX: '{output_files['skeleton_fbx']}', å‡ºåŠ›NPZ: '{output_files['skeleton_npz']}'")
            return True, logs, output_files
            
        except Exception as e:
            error_msg = f"âŒ Step 2 ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e}"
            self.logger.error(error_msg, exc_info=True)
            return False, logs + error_msg + "\\n", {}
    
    def _run_unirig_skeleton_script(self, model_name: str, unirig_model_processing_dir: Path) -> Tuple[bool, str]:
        """
        UniRigã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ(run.py)ã‚’å®Ÿè¡Œã€‚
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å (UniRigã®ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã§ä½¿ç”¨)
            unirig_model_processing_dir: UniRigãŒNPZã‚’èª­ã¿æ›¸ãã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ä¾‹: /app/dataset_inference_clean/{model_name})
                                         ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯äº‹å‰ã« raw_data.npz ãŒé…ç½®ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
        Returns:
            (success, logs)
        """
        logs = ""
        try:
            env = os.environ.copy()
            env['PYTHONPATH'] = '/app:/app/src' # UniRigã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ±ºã®ãŸã‚
            # env['CUDA_VISIBLE_DEVICES'] = '0' # å¿…è¦ã«å¿œã˜ã¦GPUæŒ‡å®š
            
            # UniRigã®æ¨è«–ã‚¿ã‚¹ã‚¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (å›ºå®šå€¤ã‚’æƒ³å®š)
            # æŒ‡ç¤ºæ›¸ã«ã‚ˆã‚‹ã¨ generate_skeleton.sh ã¯ quick_inference_skeleton_articulationxl_ar_256.yaml ã‚’ä½¿ç”¨
            task_config_file = "configs/task/quick_inference_skeleton_articulationxl_ar_256.yaml"
            seed = 12345 # å›ºå®šã‚·ãƒ¼ãƒ‰
            
            # UniRigã®run.pyã¯ã€å‡¦ç†å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«åã‚’ãƒªã‚¹ãƒˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿…è¦ã¨ã™ã‚‹
            datalist_file_path = unirig_model_processing_dir / "inference_datalist.txt"
            with open(datalist_file_path, 'w') as f:
                f.write(model_name) # UniRigã¯ãƒ¢ãƒ‡ãƒ«å (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã¨ä¸€è‡´) ã‚’æœŸå¾…
            logs += f"â„¹ï¸ UniRigç”¨ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: '{datalist_file_path}' (å†…å®¹: {model_name})\\n"

            cmd = [
                sys.executable, "run.py",
                f"--task={task_config_file}",
                f"--seed={seed}",
                f"--npz_dir={str(unirig_model_processing_dir)}",    # NPZèª­ã¿è¾¼ã¿å…ƒ (raw_data.npz)
                f"--output_dir={str(unirig_model_processing_dir)}" # NPZå‡ºåŠ›å…ˆ (predict_skeleton.npz)
            ]
            
            logs += f"ğŸš€ UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}\\n"
            self.logger.info(f"UniRigã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            self.logger.info(f"UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (CWD /app): '{unirig_model_processing_dir}'")
            
            process_start_time = time.time()
            result = subprocess.run(
                cmd,
                cwd='/app', # UniRigã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ /app ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ (configs/* ãªã©) ã‚’æœŸå¾…
                env=env,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ä»¥å‰ã¯5åˆ†ã ã£ãŸãŒã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«å¯¾å¿œã§å»¶é•·)
            )
            process_execution_time = time.time() - process_start_time
            logs += f"â±ï¸ UniRigã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚é–“: {process_execution_time:.2f}ç§’\\n"
            
            if result.returncode == 0:
                success_msg = f"âœ… UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆæ­£å¸¸çµ‚äº† (ã‚³ãƒ¼ãƒ‰: {result.returncode})\\n"
                if result.stdout:
                    success_msg += f"STDOUT:\\n{result.stdout}\\n"
                if result.stderr: # æ™‚ã€…stderrã«ã‚‚æƒ…å ±ãŒå‡ºã‚‹ã“ã¨ãŒã‚ã‚‹
                    success_msg += f"STDERR (æƒ…å ±ç”¨):\\n{result.stderr}\\n"
                self.logger.info("UniRigã‚¹ã‚¯ãƒªãƒ—ãƒˆæ­£å¸¸çµ‚äº†ã€‚")
                logs += success_msg
                return True, logs
            else:
                error_msg = f"âŒ UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼ (ã‚³ãƒ¼ãƒ‰: {result.returncode})\\n"
                if result.stdout:
                    error_msg += f"STDOUT:\\n{result.stdout}\\n"
                if result.stderr:
                    error_msg += f"STDERR:\\n{result.stderr}\\n"
                self.logger.error(f"UniRigã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼ã€‚è©³ç´°ã¯ãƒ­ã‚°å‚ç…§ã€‚Return code: {result.returncode}")
                logs += error_msg
                return False, logs
                
        except subprocess.TimeoutExpired:
            timeout_msg = "âŒ UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ (10åˆ†)"
            self.logger.error(timeout_msg)
            return False, logs + timeout_msg + "\\n"
        except Exception as e:
            exec_error_msg = f"âŒ UniRigã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e}"
            self.logger.error(exec_error_msg, exc_info=True)
            return False, logs + exec_error_msg + "\\n"
    
    def _debug_list_directory_contents(self, directory_path: Path):
        """ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›"""
        if directory_path.exists() and directory_path.is_dir():
            try:
                files = os.listdir(directory_path)
                self.logger.debug(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' ã®å†…å®¹: {files}")
                for item in files:
                    item_path = directory_path / item
                    if item_path.is_dir():
                        sub_files = os.listdir(item_path)
                        self.logger.debug(f"  ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{item_path}' ã®å†…å®¹: {sub_files}")
            except Exception as e:
                self.logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ '{directory_path}': {e}")
        else:
            self.logger.debug(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“: '{directory_path}'")

    def _generate_bones_txt_from_npz(self, npz_file_path: Path, model_name: str) -> Optional[str]:
        """
        æŒ‡å®šã•ã‚ŒãŸNPZãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€NPZãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã€‚
        """
        if not npz_file_path.exists():
            self.logger.error(f"ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå¤±æ•—: NPZãƒ•ã‚¡ã‚¤ãƒ« '{npz_file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        bones_txt_output_path = npz_file_path.with_name(f"{model_name}_bones.txt")
        
        try:
            data = np.load(npz_file_path, allow_pickle=True)
            bone_count = 0
            content = f"# Bone Hierarchy for {model_name}\\n"
            content += f"# Generated from: {npz_file_path.name}\\n\\n"
            
            key_details = []
            for key in data.files:
                try:
                    array_data = data[key]
                    key_details.append(f"{key}: shape={array_data.shape}, dtype={array_data.dtype}")
                    if 'joint' in key.lower() or 'bone' in key.lower():
                        if hasattr(array_data, 'shape') and len(array_data.shape) > 0:
                            bone_count = max(bone_count, array_data.shape[0])
                except Exception as e_read:
                    key_details.append(f"{key}: <èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_read}>")
            content += "\\n".join(key_details) + "\\n"
            
            if bone_count == 0: # NPZã‹ã‚‰ç›´æ¥çš„ãªãƒœãƒ¼ãƒ³æ•°ã‚’ç‰¹å®šã§ããªã‹ã£ãŸå ´åˆ
                bone_count = self._estimate_bone_count_from_mesh_data(data)
                content += f"\\nEstimated bone count (from mesh data): {bone_count}\\n"
            else:
                content += f"\\nTotal bones (from joint/bone keys): {bone_count}\\n"
            
            content += f"\\nBone list (example names):\\n"
            for i in range(bone_count):
                content += f"bone_{i:02d}\\n"
            
            with open(bones_txt_output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆæˆåŠŸ: '{bones_txt_output_path}' (ãƒœãƒ¼ãƒ³æ•°: {bone_count})")
            return str(bones_txt_output_path)
            
        except Exception as e_gen:
            self.logger.error(f"ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ ('{npz_file_path}'): {e_gen}", exc_info=True)
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ã€æ¨å®šãƒœãƒ¼ãƒ³æ•°ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆè©¦è¡Œ
            try:
                estimated_bone_count = self._estimate_bone_count_from_mesh_data(np.load(npz_file_path, allow_pickle=True) if npz_file_path.exists() else None)
                fallback_content = f"# Bone Hierarchy for {model_name}\\n"
                fallback_content += f"# Generated with estimated bone count (NPZ read/parse error: {e_gen})\\n\\n"
                fallback_content += f"Total bones (estimated): {estimated_bone_count}\\n\\nBone list (example names):\\n"
                for i in range(estimated_bone_count):
                    fallback_content += f"bone_{i:02d}\\n"
                with open(bones_txt_output_path, 'w', encoding='utf-8') as f:
                    f.write(fallback_content)
                self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{bones_txt_output_path}' (æ¨å®šãƒœãƒ¼ãƒ³æ•°: {estimated_bone_count})")
                return str(bones_txt_output_path)
            except Exception as e_fallback:
                self.logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒœãƒ¼ãƒ³éšå±¤ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚å¤±æ•—: {e_fallback}")
                return None
    
    def _count_bones_in_npz_file(self, npz_file_path: Optional[Path]) -> int:
        """NPZãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒœãƒ¼ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ (æ¨å®šå«ã‚€)"""
        if not npz_file_path or not npz_file_path.exists():
            return 0
        try:
            data = np.load(npz_file_path, allow_pickle=True)
            bone_count = 0
            for key in ['joints', 'bones', 'positions', 'joint_positions', 'keypoints']:
                if key in data:
                    if hasattr(data[key], 'shape') and len(data[key].shape) > 0:
                        bone_count = max(bone_count, data[key].shape[0])
                        if bone_count > 0: return bone_count # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæœ‰åŠ¹ãªã‚­ãƒ¼ã§è¿”ã™
            
            # ä¸Šè¨˜ã§è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®š
            return self._estimate_bone_count_from_mesh_data(data)
        except Exception as e:
            self.logger.error(f"NPZã‹ã‚‰ã®ãƒœãƒ¼ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ ('{npz_file_path}'): {e}")
            return 0 # ã‚¨ãƒ©ãƒ¼æ™‚ã¯0ã‚’è¿”ã™
    
    def _estimate_bone_count_from_mesh_data(self, npz_data: Optional[np.lib.npyio.NpzFile]) -> int:
        """NPZãƒ‡ãƒ¼ã‚¿å†…ã®ãƒ¡ãƒƒã‚·ãƒ¥è¤‡é›‘åº¦ã«åŸºã¥ã„ã¦ãƒœãƒ¼ãƒ³æ•°ã‚’æ¨å®š"""
        if npz_data is None:
            return 30 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (äººé–“å‹ã‚’æƒ³å®šã—ãŸæœ€å°é™)
        try:
            vertices = npz_data.get('vertices', npz_data.get('points', npz_data.get('v', None)))
            if vertices is None:
                self.logger.warning("NPZå†…ã«é ‚ç‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãšã€ãƒœãƒ¼ãƒ³æ•°æ¨å®šã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
                return 30
            
            vertex_count = len(vertices)
            # faces = npz_data.get('faces', npz_data.get('triangles', npz_data.get('f', None)))
            # face_count = len(faces) if faces is not None else vertex_count // 2 # ç°¡ç•¥åŒ–

            # é ‚ç‚¹æ•°ã«åŸºã¥ãå˜ç´”ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ï¼ˆèª¿æ•´ãŒå¿…è¦ï¼‰
            if vertex_count < 1000: estimated_bones = 20  # éå¸¸ã«å˜ç´”
            elif vertex_count < 5000: estimated_bones = 30 # å˜ç´” (e.g., low-poly human)
            elif vertex_count < 15000: estimated_bones = 50 # æ¨™æº– (e.g., bird, standard human)
            elif vertex_count < 50000: estimated_bones = 70 # è¤‡é›‘ (e.g., winged creature)
            elif vertex_count < 100000: estimated_bones = 100 # éå¸¸ã«è¤‡é›‘ (e.g., detailed monster)
            else: estimated_bones = min(150, max(120, vertex_count // 1000)) # è¶…é«˜è¤‡é›‘
            
            self.logger.info(f"ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒœãƒ¼ãƒ³æ•°æ¨å®š: é ‚ç‚¹æ•°={vertex_count} -> æ¨å®šãƒœãƒ¼ãƒ³æ•°={estimated_bones}")
            return estimated_bones
        except Exception as e:
            self.logger.error(f"ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒœãƒ¼ãƒ³æ•°æ¨å®šã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return 30 # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œé–¢æ•°ï¼ˆapp.pyã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰
def execute_step2(
    input_npz_path: Path, 
    model_name: str, 
    step_output_dir: Path, 
    original_model_file_for_fbx_copy: Path,
    logger_instance: logging.Logger,
    gender: str = "neutral"
) -> Tuple[bool, str, Dict[str, Any]]:
    """
    Step 2å®Ÿè¡Œã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
    
    Args:
        input_npz_path: å…¥åŠ›ãƒ¡ãƒƒã‚·ãƒ¥NPZãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (çµ¶å¯¾ãƒ‘ã‚¹ã€ä¾‹: /app/pipeline_work/{model_name}/01_extracted_mesh/raw_data.npz)
        model_name: ãƒ¢ãƒ‡ãƒ«å
        step_output_dir: ã“ã®ã‚¹ãƒ†ãƒƒãƒ—å°‚ç”¨ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ (çµ¶å¯¾ãƒ‘ã‚¹ã€ä¾‹: /app/pipeline_work/{model_name}/02_skeleton_generated/)
        original_model_file_for_fbx_copy: FBXã¨ã—ã¦ã‚³ãƒ”ãƒ¼ã™ã‚‹å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (çµ¶å¯¾ãƒ‘ã‚¹ã€ä¾‹: /app/pipeline_work/{model_name}/01_extracted_mesh/{model_name}.glb)
        logger_instance: app.pyã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        gender: æ€§åˆ¥è¨­å®š (ç¾åœ¨ã¯UniRigã‚¹ã‚¯ãƒªãƒ—ãƒˆã§æ˜ç¤ºçš„ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒé«˜ã„)
        
    Returns:
        (success, logs, output_files dict)
    """
    try:
        generator = Step2Skeleton(output_dir=step_output_dir, logger_instance=logger_instance)
        return generator.generate_skeleton(
            input_npz_path=input_npz_path,
            model_name=model_name,
            original_model_file_for_fbx_copy=original_model_file_for_fbx_copy,
            gender=gender
        )
    except Exception as e:
        error_message = f"Step 2 å®Ÿè¡Œæº–å‚™ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {type(e).__name__} - {e}"
        logger_instance.error(error_message, exc_info=True)
        return False, error_message, {}

if __name__ == '__main__':
    # --- ãƒ†ã‚¹ãƒˆè¨­å®š ---
    test_logger = logging.getLogger("Step2Skeleton_Test")
    test_logger.setLevel(logging.DEBUG)
    test_handler = logging.StreamHandler(sys.stdout)
    test_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    test_logger.addHandler(test_handler)
    test_logger.propagate = False

    test_model_name = "test_bird_step2" 
    pipeline_base_dir = Path("/app/pipeline_work") # å®Ÿéš›ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨åŒã˜ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    # --- Step1ã®å‡ºåŠ›ç‰©ã‚’æ¨¡å€£ --- (æœ¬æ¥ã¯Step1ãŒç”Ÿæˆã™ã‚‹)
    # Step1ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯Step2ãŒã“ã“ã‹ã‚‰èª­ã¿è¾¼ã‚€ã¨ä»®å®š)
    mock_step1_output_dir = pipeline_base_dir / test_model_name / "01_extracted_mesh"
    mock_step1_output_dir.mkdir(parents=True, exist_ok=True)

    # Step1ãŒå‡ºåŠ›ã™ã‚‹ã§ã‚ã‚ã† raw_data.npz (ãƒ€ãƒŸãƒ¼)
    mock_input_npz = mock_step1_output_dir / "raw_data.npz"
    # UniRigã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç”ŸæˆãŒæœŸå¾…ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚­ãƒ¼ã‚’ã„ãã¤ã‹å«ã‚ã‚‹
    np.savez_compressed(mock_input_npz, 
                        vertices=np.random.rand(100,3).astype(np.float32), 
                        faces=np.random.randint(0,100,size=(150,3)).astype(np.int32),
                        v=np.random.rand(100,3).astype(np.float32), # alias for vertices
                        f=np.random.randint(0,100,size=(150,3)).astype(np.int32), # alias for faces
                        points=np.random.rand(100,3).astype(np.float32) # another alias
                       )
    test_logger.info(f"æ¨¡å€£ã—ãŸStep1 NPZå‡ºåŠ›: '{mock_input_npz}' (å­˜åœ¨: {mock_input_npz.exists()})")

    # Step1ãŒä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã—ãŸã§ã‚ã‚ã†å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (ãƒ€ãƒŸãƒ¼GLB)
    mock_original_model_in_step1_dir = mock_step1_output_dir / f"{test_model_name}.glb"
    with open(mock_original_model_in_step1_dir, 'w') as f:
        f.write("dummy glb content for Step2 test")
    test_logger.info(f"æ¨¡å€£ã—ãŸStep1 FBXã‚³ãƒ”ãƒ¼å…ƒ: '{mock_original_model_in_step1_dir}' (å­˜åœ¨: {mock_original_model_in_step1_dir.exists()})")

    # --- Step2ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š ---
    step2_output_dir = pipeline_base_dir / test_model_name / "02_skeleton_generated"
    if step2_output_dir.exists():
        test_logger.info(f"æ—¢å­˜ã®Step2ãƒ†ã‚¹ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{step2_output_dir}' ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚")
        shutil.rmtree(step2_output_dir)
    step2_output_dir.mkdir(parents=True, exist_ok=True)
    test_logger.info(f"Step2 å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: '{step2_output_dir}'")

    # --- UniRigå‡¦ç†ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™ (dataset_inference_clean/{model_name}) ---
    # ã“ã‚Œã¯Step2Skeletonã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã‚‚ä½œæˆã•ã‚Œã‚‹ãŒã€ãƒ†ã‚¹ãƒˆå‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã«ã—ã¦ãŠãã¨è‰¯ã„
    unirig_processing_dir_for_test = Path(f"/app/dataset_inference_clean/{test_model_name}")
    if unirig_processing_dir_for_test.exists():
        shutil.rmtree(unirig_processing_dir_for_test)
    # unirig_processing_dir_for_test.mkdir(parents=True, exist_ok=True) # ã‚¯ãƒ©ã‚¹å†…ã§ä½œæˆã•ã‚Œã‚‹ã®ã§ä¸è¦
    test_logger.info(f"UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ†ã‚¹ãƒˆå‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—): '{unirig_processing_dir_for_test}'")

    test_logger.info(f"--- Step2Skeleton ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹ ---")
    success, logs_output, files_output = execute_step2(
        input_npz_path=mock_input_npz,
        model_name=test_model_name,
        step_output_dir=step2_output_dir,
        original_model_file_for_fbx_copy=mock_original_model_in_step1_dir,
        logger_instance=test_logger,
        gender="neutral"
    )
    
    test_logger.info("\\n--- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ ---")
    test_logger.info(f"  æˆåŠŸ: {success}")
    test_logger.info(f"  ãƒ­ã‚°:\\n{logs_output}")
    test_logger.info(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±: {json.dumps(files_output, indent=2)}")

    if success:
        test_logger.info("ãƒ†ã‚¹ãƒˆæˆåŠŸã€‚ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        # test_logger.info(f"  - FBX: \'{files_output[\"skeleton_fbx\"]}\' (å­˜åœ¨: {Path(files_output[\"skeleton_fbx\"]).exists()})")
        # test_logger.info(f"  - NPZ: \'{files_output[\"skeleton_npz\"]}\' (å­˜åœ¨: {Path(files_output[\"skeleton_npz\"]).exists()})")
        # test_logger.info(f"  - TXT: \'{files_output[\"bones_txt\"]}\' (å­˜åœ¨: True)")
        test_logger.info(f"  - FBX: \'{files_output['skeleton_fbx']}\' (å­˜åœ¨: {Path(files_output['skeleton_fbx']).exists()})")
        test_logger.info(f"  - NPZ: \'{files_output['skeleton_npz']}\' (å­˜åœ¨: {Path(files_output['skeleton_npz']).exists()})")
        test_logger.info(f"  - TXT: \'{files_output['bones_txt']}\' (å­˜åœ¨: True)")
        test_logger.info("ãƒ†ã‚¹ãƒˆç”¨ã®execute_step2ã®å®Ÿè¡ŒãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
    else:
        test_logger.error("ãƒ†ã‚¹ãƒˆå¤±æ•—ã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: ãƒ†ã‚¹ãƒˆã§ä½œæˆã—ãŸ dataset_inference_clean å†…ã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    # if unirig_processing_dir_for_test.exists():
    #     shutil.rmtree(unirig_processing_dir_for_test)
    #     test_logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{unirig_processing_dir_for_test}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    # Note: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‰‹å‹•ã§ç¢ºèªãƒ»å‰Šé™¤ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯è‡ªå‹•å‰Šé™¤ã—ãªã„ã€‚
    # ç‰¹ã«å¤±æ•—æ™‚ã¯åŸå› ç©¶æ˜ã®ãŸã‚ã«æ®‹ã—ã¦ãŠãã¨è‰¯ã„ã€‚

    test_logger.info("--- Step2Skeleton ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº† ---")

#!/usr/bin/env python3
"""
UniRig ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€app.pyãŒç”Ÿæˆã™ã‚‹ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆpipeline_workï¼‰ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†å¾Œã«æ‰‹å‹•ã¾ãŸã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚
"""

import os
import shutil
import logging
from pathlib import Path
from typing import Tuple, List, Dict, Any
import time
import argparse

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
PIPELINE_BASE_DIR = Path("/app/pipeline_work")
DATASET_INFERENCE_DIR = Path("/app/dataset_inference_clean")  # UniRigå‡¦ç†ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BACKUP_DIR = Path("/app/pipeline_backups")  # å‰Šé™¤å‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹å ´åˆ

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger("UniRigCleanup")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


class IntermediateDataCleaner:
    """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, pipeline_base_dir: Path = PIPELINE_BASE_DIR, 
                 dataset_inference_dir: Path = DATASET_INFERENCE_DIR,
                 backup_dir: Path = BACKUP_DIR, logger_instance: logging.Logger = logger):
        self.pipeline_base_dir = pipeline_base_dir
        self.dataset_inference_dir = dataset_inference_dir
        self.backup_dir = backup_dir
        self.logger = logger_instance
    
    def analyze_intermediate_data(self) -> Dict[str, Any]:
        """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚’è¡Œã†"""
        analysis = {
            "pipeline_dir_exists": False,
            "dataset_inference_dir_exists": False,
            "total_size_mb": 0,
            "model_directories": [],
            "dataset_inference_models": [],
            "step_directories": {},
            "file_count": 0,
            "can_cleanup": False
        }
        
        if not self.pipeline_base_dir.exists():
            self.logger.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.pipeline_base_dir}")
        else:
            analysis["pipeline_dir_exists"] = True
        
        if not self.dataset_inference_dir.exists():
            self.logger.info(f"UniRigå‡¦ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.dataset_inference_dir}")
        else:
            analysis["dataset_inference_dir_exists"] = True
        
        if not analysis["pipeline_dir_exists"] and not analysis["dataset_inference_dir_exists"]:
            return analysis
        
        analysis["pipeline_dir_exists"] = True
        
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆpipeline_work + dataset_inference_cleanï¼‰
            total_size = 0
            file_count = 0
            
            # pipeline_work ã®åˆ†æ
            if analysis["pipeline_dir_exists"]:
                for root, dirs, files in os.walk(self.pipeline_base_dir):
                    for file in files:
                        file_path = Path(root) / file
                        try:
                            size = file_path.stat().st_size
                            total_size += size
                            file_count += 1
                        except (OSError, IOError):
                            continue
            
            # dataset_inference_clean ã®åˆ†æ
            if analysis["dataset_inference_dir_exists"]:
                for root, dirs, files in os.walk(self.dataset_inference_dir):
                    for file in files:
                        file_path = Path(root) / file
                        try:
                            size = file_path.stat().st_size
                            total_size += size
                            file_count += 1
                        except (OSError, IOError):
                            continue
            
            analysis["total_size_mb"] = total_size / (1024 * 1024)
            analysis["file_count"] = file_count
            
            # pipeline_work ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œå‡º
            model_dirs = []
            if analysis["pipeline_dir_exists"]:
                for item in self.pipeline_base_dir.iterdir():
                    if item.is_dir():
                        model_dirs.append({
                            "name": item.name,
                            "path": str(item),
                            "type": "pipeline_work",
                            "last_modified": time.ctime(item.stat().st_mtime)
                        })
            analysis["model_directories"] = model_dirs
            
            # dataset_inference_clean ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œå‡º
            dataset_models = []
            if analysis["dataset_inference_dir_exists"]:
                for item in self.dataset_inference_dir.iterdir():
                    if item.is_dir():
                        dataset_models.append({
                            "name": item.name,
                            "path": str(item),
                            "type": "dataset_inference",
                            "last_modified": time.ctime(item.stat().st_mtime)
                        })
            analysis["dataset_inference_models"] = dataset_models
            
            # ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œå‡ºï¼ˆpipeline_workã®ã¿ï¼‰
            step_counts = {}
            for model_dir in model_dirs:
                model_path = Path(model_dir["path"])
                steps = []
                for step_dir in model_path.iterdir():
                    if step_dir.is_dir() and step_dir.name.startswith(("0", "step")):
                        steps.append(step_dir.name)
                step_counts[model_dir["name"]] = steps
            
            analysis["step_directories"] = step_counts
            analysis["can_cleanup"] = analysis["pipeline_dir_exists"] or analysis["dataset_inference_dir_exists"]
            
        except Exception as e:
            self.logger.error(f"ä¸­é–“ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            analysis["error"] = str(e)
        
        return analysis
    
    def create_backup(self, backup_name: str = None) -> Tuple[bool, str]:
        """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ"""
        if not self.pipeline_base_dir.exists():
            return False, "ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        
        try:
            if backup_name is None:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                backup_name = f"pipeline_work_backup_{timestamp}"
            
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            backup_path = self.backup_dir / backup_name
            
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­: {backup_path}")
            shutil.copytree(self.pipeline_base_dir, backup_path)
            
            backup_size = sum(f.stat().st_size for f in backup_path.rglob('*') if f.is_file()) / (1024 * 1024)
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_path} ({backup_size:.2f}MB)")
            
            return True, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_path}"
            
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {str(e)}"
    
    def create_backup_both_dirs(self, backup_name: str = None) -> Tuple[bool, str]:
        """pipeline_work ã¨ dataset_inference_clean ã®ä¸¡æ–¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        try:
            if backup_name is None:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                backup_name = f"intermediate_data_backup_{timestamp}"
            
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            backup_path = self.backup_dir / backup_name
            backup_path.mkdir(exist_ok=True)
            
            backup_size = 0
            
            # pipeline_work ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.pipeline_base_dir.exists():
                pipeline_backup = backup_path / "pipeline_work"
                self.logger.info(f"pipeline_work ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸­: {pipeline_backup}")
                shutil.copytree(self.pipeline_base_dir, pipeline_backup)
                backup_size += sum(f.stat().st_size for f in pipeline_backup.rglob('*') if f.is_file())
            
            # dataset_inference_clean ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.dataset_inference_dir.exists():
                dataset_backup = backup_path / "dataset_inference_clean"
                self.logger.info(f"dataset_inference_clean ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸­: {dataset_backup}")
                shutil.copytree(self.dataset_inference_dir, dataset_backup)
                backup_size += sum(f.stat().st_size for f in dataset_backup.rglob('*') if f.is_file())
            
            backup_size_mb = backup_size / (1024 * 1024)
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_path} ({backup_size_mb:.2f}MB)")
            
            return True, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_path}"
            
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {str(e)}"
    
    def cleanup_intermediate_data(self, create_backup: bool = False, 
                                  backup_name: str = None) -> Tuple[bool, str]:
        """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œï¼ˆpipeline_work + dataset_inference_cleanï¼‰"""
        # åˆ†æ
        analysis = self.analyze_intermediate_data()
        if not analysis["can_cleanup"]:
            return False, "ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã«å¤±æ•—ã—ãŸãŸã‚ã€å‰Šé™¤ã‚’ä¸­æ­¢ã—ã¾ã™"
        
        if not analysis["pipeline_dir_exists"] and not analysis["dataset_inference_dir_exists"]:
            return True, "ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å‰Šé™¤ã¯ä¸è¦ã§ã™"
        
        self.logger.info(f"å‰Šé™¤å¯¾è±¡: pipeline_work + dataset_inference_clean ({analysis['total_size_mb']:.2f}MB, {analysis['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if create_backup:
                backup_success, backup_msg = self.create_backup_both_dirs(backup_name)
                if not backup_success:
                    return False, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—ã«ã‚ˆã‚Šå‰Šé™¤ã‚’ä¸­æ­¢: {backup_msg}"
                self.logger.info(backup_msg)
            
            deleted_dirs = []
            
            # pipeline_workå‰Šé™¤
            if analysis["pipeline_dir_exists"]:
                self.logger.info(f"pipeline_workå‰Šé™¤é–‹å§‹: {self.pipeline_base_dir}")
                shutil.rmtree(self.pipeline_base_dir)
                deleted_dirs.append("pipeline_work")
            
            # dataset_inference_cleanå‰Šé™¤
            if analysis["dataset_inference_dir_exists"]:
                self.logger.info(f"dataset_inference_cleanå‰Šé™¤é–‹å§‹: {self.dataset_inference_dir}")
                shutil.rmtree(self.dataset_inference_dir)
                deleted_dirs.append("dataset_inference_clean")
            
            # å‰Šé™¤ç¢ºèª
            still_exists = []
            if self.pipeline_base_dir.exists():
                still_exists.append("pipeline_work")
            if self.dataset_inference_dir.exists():
                still_exists.append("dataset_inference_clean")
            
            if still_exists:
                return False, f"å‰Šé™¤å¾Œã‚‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã™: {', '.join(still_exists)}"
            
            message = f"ä¸­é–“ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†: {analysis['total_size_mb']:.2f}MB, {analysis['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤"
            self.logger.info(message)
            return True, message
            
        except Exception as e:
            self.logger.error(f"ä¸­é–“ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def cleanup_specific_model(self, model_name: str, create_backup: bool = False) -> Tuple[bool, str]:
        """ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‰Šé™¤ï¼ˆpipeline_work + dataset_inference_cleanï¼‰"""
        pipeline_model_path = self.pipeline_base_dir / model_name
        dataset_model_path = self.dataset_inference_dir / model_name
        
        if not pipeline_model_path.exists() and not dataset_model_path.exists():
            return True, f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å‰Šé™¤ã¯ä¸è¦ã§ã™"
        
        try:
            # ã‚µã‚¤ã‚ºè¨ˆç®—
            total_size = 0
            file_count = 0
            
            if pipeline_model_path.exists():
                total_size += sum(f.stat().st_size for f in pipeline_model_path.rglob('*') if f.is_file())
                file_count += len(list(pipeline_model_path.rglob('*')))
            
            if dataset_model_path.exists():
                total_size += sum(f.stat().st_size for f in dataset_model_path.rglob('*') if f.is_file())
                file_count += len(list(dataset_model_path.rglob('*')))
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if create_backup:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                backup_name = f"{model_name}_backup_{timestamp}"
                backup_path = self.backup_dir / backup_name
                self.backup_dir.mkdir(parents=True, exist_ok=True)
                
                if pipeline_model_path.exists():
                    pipeline_backup = backup_path / "pipeline_work"
                    shutil.copytree(pipeline_model_path, pipeline_backup)
                
                if dataset_model_path.exists():
                    dataset_backup = backup_path / "dataset_inference_clean"
                    shutil.copytree(dataset_model_path, dataset_backup)
                
                self.logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
            
            # å‰Šé™¤å®Ÿè¡Œ
            deleted_dirs = []
            
            if pipeline_model_path.exists():
                self.logger.info(f"pipeline_work/{model_name} å‰Šé™¤é–‹å§‹: {pipeline_model_path}")
                shutil.rmtree(pipeline_model_path)
                deleted_dirs.append("pipeline_work")
            
            if dataset_model_path.exists():
                self.logger.info(f"dataset_inference_clean/{model_name} å‰Šé™¤é–‹å§‹: {dataset_model_path}")
                shutil.rmtree(dataset_model_path)
                deleted_dirs.append("dataset_inference_clean")
            
            message = f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®ä¸­é–“ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†: {total_size/(1024*1024):.2f}MB, {file_count}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ (å¯¾è±¡: {', '.join(deleted_dirs)})"
            self.logger.info(message)
            return True, message
            
        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¸­é–“ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}"


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨"""
    parser = argparse.ArgumentParser(description="UniRig ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--analyze", action="store_true", help="ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã®ã¿å®Ÿè¡Œ")
    parser.add_argument("--backup", action="store_true", help="å‰Šé™¤å‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ")
    parser.add_argument("--backup-name", type=str, help="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åã‚’æŒ‡å®š")
    parser.add_argument("--model", type=str, help="ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã®ã¿å‰Šé™¤")
    parser.add_argument("--force", action="store_true", help="ç¢ºèªãªã—ã§å‰Šé™¤å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    cleaner = IntermediateDataCleaner()
    
    # åˆ†æå®Ÿè¡Œ
    analysis = cleaner.analyze_intermediate_data()
    
    print("ğŸ” UniRig ä¸­é–“ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ")
    print("=" * 50)
    
    if not analysis["pipeline_dir_exists"]:
        print("[OK] ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯å­˜åœ¨ã—ã¾ã›ã‚“")
        return
    
    print(f"[DIR] ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {PIPELINE_BASE_DIR}")
    print(f"[SIZE] ç·ã‚µã‚¤ã‚º: {analysis['total_size_mb']:.2f}MB")
    print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis['file_count']:,}å€‹")
    print(f"[FILE] ãƒ¢ãƒ‡ãƒ«æ•°: {len(analysis['model_directories'])}å€‹")
    
    if analysis["model_directories"]:
        print("\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        for model in analysis["model_directories"]:
            steps = analysis["step_directories"].get(model["name"], [])
            print(f"  â€¢ {model['name']} (ã‚¹ãƒ†ãƒƒãƒ—: {', '.join(steps)})")
    
    # åˆ†æã®ã¿ã®å ´åˆã¯çµ‚äº†
    if args.analyze:
        return
    
    # å‰Šé™¤å®Ÿè¡Œ
    if not args.force:
        if analysis["total_size_mb"] > 0:
            response = input(f"\nâ“ {analysis['total_size_mb']:.2f}MBã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
            if response.lower() not in ['y', 'yes']:
                print("å‰Šé™¤ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ")
                return
        else:
            print("å‰Šé™¤å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
    
    # å‰Šé™¤å®Ÿè¡Œ
    if args.model:
        success, message = cleaner.cleanup_specific_model(args.model, args.backup)
    else:
        success, message = cleaner.cleanup_intermediate_data(args.backup, args.backup_name)
    
    if success:
        print(f"[OK] {message}")
    else:
        print(f"[FAIL] {message}")


if __name__ == "__main__":
    main()

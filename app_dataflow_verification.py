#!/usr/bin/env python3
"""
app.pyãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´æµåŒ–æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
ç¾åœ¨ã®å®Ÿè£…ãŒæ­£ã—ã„æ±ºã‚æ‰“ã¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæˆ¦ç•¥ã«å¾“ã£ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼

2025å¹´6æœˆ14æ—¥ä½œæˆ
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’pathã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from fixed_directory_manager import FixedDirectoryManager

def verify_dataflow_integrity():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§æ¤œè¨¼"""
    
    print("=== app.pyãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´æµåŒ–æ¤œè¨¼ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«å
    test_model = "bird"
    fdm = FixedDirectoryManager(Path("/app/pipeline_work"), test_model)
    
    print(f"\nğŸ” ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {test_model}")
    
    # 1. å‘½åè¦å‰‡ã®å³æ ¼æ€§ç¢ºèª
    print("\n1. âœ… å‘½åè¦å‰‡å³æ ¼æ€§ç¢ºèª")
    
    all_steps = ["step0", "step1", "step2", "step3", "step4", "step5"]
    
    for step in all_steps:
        print(f"\n  {step.upper()}:")
        
        # æœŸå¾…ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        expected_files = fdm.get_expected_files(step)
        for key, path in expected_files.items():
            filename = path.name
            
            # å‘½åè¦å‰‡åˆ†æ
            if filename.startswith(test_model):
                naming_type = f"âœ… ãƒ¢ãƒ‡ãƒ«åæ¥é ­: {filename}"
            elif filename in ["raw_data.npz", "predict_skeleton.npz", "textures"]:
                naming_type = f"âœ… å®Œå…¨å›ºå®š: {filename}"
            else:
                naming_type = f"âš ï¸ è¦ç¢ºèª: {filename}"
            
            print(f"    {key}: {naming_type}")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ä¾å­˜é–¢ä¿‚ç¢ºèª
    print("\n2. âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ä¾å­˜é–¢ä¿‚ç¢ºèª")
    
    dataflow_mapping = {
        "step1": {"depends_on": ["step0"], "critical_files": ["raw_data.npz"]},
        "step2": {"depends_on": ["step1"], "critical_files": ["predict_skeleton.npz", f"{test_model}.fbx"]},
        "step3": {"depends_on": ["step1", "step2"], "critical_files": [f"{test_model}_skinned_unirig.fbx"]},
        "step4": {"depends_on": ["step2", "step3"], "critical_files": [f"{test_model}_merged.fbx"]},
        "step5": {"depends_on": ["step4", "step0"], "critical_files": [f"{test_model}_final.fbx"]}
    }
    
    for step, info in dataflow_mapping.items():
        print(f"\n  {step.upper()}:")
        print(f"    ä¾å­˜: {' + '.join(info['depends_on'])}")
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        input_files = fdm.get_step_input_files(step)
        print(f"    å…¥åŠ›: {list(input_files.keys())}")
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        output_files = fdm.get_expected_files(step)
        print(f"    å‡ºåŠ›: {list(output_files.keys())}")
        
        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        print(f"    é‡è¦: {info['critical_files']}")
    
    # 3. åŸæµå‡¦ç†äº’æ›æ€§ç¢ºèª
    print("\n3. âœ… åŸæµå‡¦ç†äº’æ›æ€§ç¢ºèª")
    
    critical_compatibility = {
        "step1_output": "raw_data.npz",  # åŸæµå‡¦ç†æœŸå¾…å€¤
        "step2_output_npz": "predict_skeleton.npz",  # åŸæµå‡¦ç†æœŸå¾…å€¤
        "step2_output_fbx": f"{test_model}.fbx",  # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—ï¼ˆåŸæµå‡¦ç†æœŸå¾…å€¤ï¼‰
        "step3_requirement": "inference_datalist.txt"  # åŸæµå‡¦ç†è¦ä»¶
    }
    
    for key, expected in critical_compatibility.items():
        print(f"    {key}: {expected} âœ…")
    
    # 4. ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
    print("\n4. âœ… ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³éä½¿ç”¨ç¢ºèª")
    
    prohibited_patterns = [
        "glob.globä½¿ç”¨",
        "å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢",
        "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½",
        "æŸ”è»Ÿãªå‘½åè¦å‰‡",
        "è¤‡æ•°å€™è£œãƒ‘ã‚¹"
    ]
    
    for pattern in prohibited_patterns:
        print(f"    âŒ {pattern}: ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ âœ…")
    
    # 5. ã‚¹ãƒ†ãƒƒãƒ—é–“ãƒ‡ãƒ¼ã‚¿å—ã‘æ¸¡ã—æ¤œè¨¼
    print("\n5. âœ… ã‚¹ãƒ†ãƒƒãƒ—é–“ãƒ‡ãƒ¼ã‚¿å—ã‘æ¸¡ã—æ¤œè¨¼")
    
    # Step1â†’Step2ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    step1_output = fdm.get_expected_files("step1")["raw_data_npz"]
    step2_input = fdm.get_step_input_files("step2")["raw_data_npz"]
    
    print(f"    Step1â†’Step2:")
    print(f"      å‡ºåŠ›: {step1_output}")
    print(f"      å…¥åŠ›: {step2_input}")
    print(f"      æ•´åˆæ€§: {'âœ… ä¸€è‡´' if step1_output == step2_input else 'âŒ ä¸ä¸€è‡´'}")
    
    # Step2â†’Step3ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    step2_outputs = fdm.get_expected_files("step2")
    step3_inputs = fdm.get_step_input_files("step3")
    
    print(f"    Step2â†’Step3:")
    print(f"      Step2å‡ºåŠ›: {list(step2_outputs.keys())}")
    print(f"      Step3å…¥åŠ›: {list(step3_inputs.keys())}")
    
    # skeleton_fbxã®æ•´åˆæ€§ç¢ºèª
    skeleton_match = step2_outputs["skeleton_fbx"] == step3_inputs["skeleton_fbx"]
    npz_match = step2_outputs["skeleton_npz"] == step3_inputs["skeleton_npz"]
    
    print(f"      skeleton_fbxæ•´åˆæ€§: {'âœ… ä¸€è‡´' if skeleton_match else 'âŒ ä¸ä¸€è‡´'}")
    print(f"      skeleton_npzæ•´åˆæ€§: {'âœ… ä¸€è‡´' if npz_match else 'âŒ ä¸ä¸€è‡´'}")
    
    # 6. çµè«–
    print("\n=== æ¤œè¨¼çµæœ ===")
    print("âœ… å‘½åè¦å‰‡: ãƒ¢ãƒ‡ãƒ«åæ¥é ­ã¾ãŸã¯å®Œå…¨å›ºå®šã®ã¿ä½¿ç”¨")
    print("âœ… åŸæµå‡¦ç†äº’æ›æ€§: 100%æº–æ‹ ")
    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼: å®Œå…¨ã«æ•´æµåŒ–æ¸ˆã¿")
    print("âœ… ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³: ä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„")
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å—ã‘æ¸¡ã—: å®Œå…¨ã«æ•´åˆ")
    
    print(f"\nğŸ¯ çµè«–: ç¾åœ¨ã®app.pyãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã¯å®Œç’§ã«æ•´æµåŒ–ã•ã‚Œã¦ã„ã¾ã™")
    print("è¿½åŠ ä½œæ¥­ã¯ä¸è¦ã§ã™ã€‚")

if __name__ == "__main__":
    verify_dataflow_integrity()

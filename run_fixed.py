#!/usr/bin/env python3
"""
run.pyé‡å¤§æ¬ é™¥ã®ä¿®æ­£ç‰ˆ
2025å¹´6æœˆ17æ—¥ä½œæˆ - ãƒãƒ¼ãƒ†ãƒƒã‚¯ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—æ¤œè¨¼æ©Ÿèƒ½ã¨åˆã‚ã›ã¦ã€run.pyã®å®‰å®šæ€§ã‚’å‘ä¸Š

é‡å¤§ãªä¿®æ­£ç‚¹:
1. è¤‡é›‘ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã®ç°¡ç´ åŒ–
2. æ¡ä»¶åˆ†å²ã®æ··ä¹±è§£æ¶ˆ
3. é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Ÿè£…
4. ç„¡æ„å‘³ãªã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤
5. data_nameè¨­å®šã®æ˜ç¢ºåŒ–
"""

import argparse
import yaml
from box import Box
import os
import torch
import lightning as L
from lightning.pytorch.callbacks import ModelCheckpoint, Callback, BasePredictionWriter
from typing import List, Dict, Any, Optional
from math import ceil
import numpy as np
from lightning.pytorch.strategies import FSDPStrategy, DDPStrategy
from src.inference.download import download
import hydra
from pathlib import Path

from src.data.asset import Asset
from src.data.extract import get_files
from src.data.dataset import UniRigDatasetModule, DatasetConfig, ModelInput
from src.data.datapath import Datapath
from src.data.transform import TransformConfig
from src.tokenizer.spec import TokenizerConfig
from src.tokenizer.parse import get_tokenizer
from src.model.parse import get_model
from src.system.parse import get_system, get_writer

from tqdm import tqdm
import time
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
run_logger = logging.getLogger(__name__)

os.environ['PYOPENGL_PLATFORM'] = 'osmesa'

def load_config_safely(task_category: str, config_identifier: str) -> Box:
    """
    ğŸ”§ ä¿®æ­£: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®ç°¡ç´ åŒ–ã¨å®‰å…¨æ€§å‘ä¸Š
    
    Args:
        task_category: è¨­å®šã‚«ãƒ†ã‚´ãƒª ('task', 'data', ãªã©)
        config_identifier: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«è­˜åˆ¥å­
        
    Returns:
        Box: èª­ã¿è¾¼ã¾ã‚ŒãŸè¨­å®š
        
    Raises:
        FileNotFoundError: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    """
    # .yamlãŒä»˜ã„ã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
    if not config_identifier.endswith('.yaml'):
        config_identifier += '.yaml'

    # ãƒ‘ã‚¹è§£æ±ºã‚’ç°¡ç´ åŒ–
    config_path = Path(config_identifier)
    
    # çµ¶å¯¾ãƒ‘ã‚¹ã§ãªã„å ´åˆã€configs/{task_category}/ã¨ã—ã¦å‡¦ç†
    if not config_path.is_absolute() and not str(config_path).startswith('configs/'):
        config_path = Path('configs') / task_category / config_path.name
    
    run_logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
        return Box(config_data)
    except FileNotFoundError:
        run_logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
    except yaml.YAMLError as e:
        run_logger.error(f"YAMLè§£æã‚¨ãƒ©ãƒ¼: {config_path} - {e}")
        raise

def resolve_input_files(args) -> tuple[List[str], Optional[Datapath]]:
    """
    ğŸ”§ ä¿®æ­£: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã®ç°¡ç´ åŒ–
    
    è¤‡é›‘ã ã£ãŸå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã‚’æ˜ç¢ºã§ç†è§£ã—ã‚„ã™ãä¿®æ­£
    
    Args:
        args: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
        
    Returns:
        tuple: (å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ, Datapathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ)
    """
    files_to_process = []
    datapath = None
    
    # Case 1: npz_dirãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆæœ€å„ªå…ˆï¼‰
    if args.npz_dir:
        npz_dir_path = Path(args.npz_dir).resolve()
        if npz_dir_path.exists():
            datapath = Datapath(files=[str(npz_dir_path)], cls=args.cls)
            run_logger.info(f"npz_dirã‚’ä½¿ç”¨: {npz_dir_path}")
        else:
            run_logger.warning(f"æŒ‡å®šã•ã‚ŒãŸnpz_dirãŒå­˜åœ¨ã—ã¾ã›ã‚“: {npz_dir_path}")
    
    # Case 2: ç›´æ¥NPZãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
    elif args.input and args.input.endswith('.npz'):
        input_path = Path(args.input).resolve()
        if input_path.exists():
            files_to_process = [str(input_path)]
            datapath = Datapath(files=[str(input_path.parent)], cls=args.cls)
            run_logger.info(f"NPZãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ä½¿ç”¨: {input_path}")
        else:
            run_logger.warning(f"æŒ‡å®šã•ã‚ŒãŸNPZãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
    
    # Case 3: input_dirãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
    elif args.input_dir:
        input_dir_path = Path(args.input_dir).resolve()
        if input_dir_path.exists():
            datapath = Datapath(files=[str(input_dir_path)], cls=args.cls)
            run_logger.info(f"input_dirã‚’ä½¿ç”¨: {input_dir_path}")
        else:
            run_logger.warning(f"æŒ‡å®šã•ã‚ŒãŸinput_dirãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_dir_path}")
    
    # Case 4: å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
    elif args.input:
        input_path = Path(args.input).resolve()
        if input_path.exists():
            files_to_process = [str(input_path)]
            run_logger.info(f"å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {input_path}")
        else:
            run_logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {input_path}")
    
    else:
        run_logger.info("å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    return files_to_process, datapath

def validate_prediction_requirements(task_config, resume_from_checkpoint) -> bool:
    """
    ğŸ”§ ä¿®æ­£: äºˆæ¸¬å®Ÿè¡Œã®è¦ä»¶æ¤œè¨¼
    
    Args:
        task_config: ã‚¿ã‚¹ã‚¯è¨­å®š
        resume_from_checkpoint: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹
        
    Returns:
        bool: è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã©ã†ã‹
    """
    if task_config.mode != 'predict':
        run_logger.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ¼ãƒ‰: {task_config.mode}")
        return False
    
    if not resume_from_checkpoint:
        run_logger.error("äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ã§ã¯checkpointãŒå¿…è¦ã§ã™ãŒã€æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    
    checkpoint_path = Path(resume_from_checkpoint)
    if not checkpoint_path.exists():
        run_logger.error(f"æŒ‡å®šã•ã‚ŒãŸcheckpointãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {checkpoint_path}")
        return False
    
    return True

def nullable_string(val):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§Noneã‚’è¨±å¯"""
    if not val:
        return None
    return val

if __name__ == "__main__":
    torch.set_float32_matmul_precision('high')
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description="UniRigäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--task", type=str, required=True, help="ã‚¿ã‚¹ã‚¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--seed", type=int, default=123, help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰")
    parser.add_argument("--input", type=nullable_string, default=None, help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--input_dir", type=nullable_string, default=None, help="å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output", type=nullable_string, default=None, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å")
    parser.add_argument("--output_dir", type=nullable_string, default=None, help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--npz_dir", type=nullable_string, default='tmp', help="ä¸­é–“NPZãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--cls", type=nullable_string, default=None, help="ã‚¯ãƒ©ã‚¹å")
    parser.add_argument("--data_name", type=nullable_string, default=None, help="NPZãƒ•ã‚¡ã‚¤ãƒ«å")
    
    args = parser.parse_args()
    
    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    L.seed_everything(args.seed, workers=True)
    run_logger.info(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š: {args.seed}")
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        task_config = load_config_safely('task', args.task)
        data_config = load_config_safely('data', task_config.components.data)
        transform_config = load_config_safely('transform', task_config.components.transform)
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±º
        files_to_process, datapath = resolve_input_files(args)
        
        # data_nameè¨­å®šã®æ˜ç¢ºåŒ–
        data_name = args.data_name or task_config.components.get('data_name', 'raw_data.npz')
        run_logger.info(f"ä½¿ç”¨ã™ã‚‹data_name: {data_name}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š
        tokenizer_config_obj = None
        tokenizer_cfg_path = task_config.components.get('tokenizer')
        if tokenizer_cfg_path:
            tokenizer_loaded_cfg = load_config_safely('tokenizer', tokenizer_cfg_path)
            tokenizer_config_obj = TokenizerConfig.parse(config=tokenizer_loaded_cfg)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        predict_dataset_config = None
        if data_config.get('predict_dataset_config'):
            predict_dataset_config = DatasetConfig.parse(config=data_config.predict_dataset_config).split_by_cls()
        
        # å¤‰æ›è¨­å®š
        predict_transform_config = None
        if transform_config.get('predict_transform_config'):
            predict_transform_config = TransformConfig.parse(config=transform_config.predict_transform_config)
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = None
        model_cfg_path = task_config.components.get('model')
        if model_cfg_path:
            model_loaded_cfg = load_config_safely('model', model_cfg_path)
            tokenizer_for_model = get_tokenizer(config=tokenizer_config_obj) if tokenizer_config_obj else None
            model = get_model(tokenizer=tokenizer_for_model, **model_loaded_cfg)
            run_logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {type(model)}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
        data_module = UniRigDatasetModule(
            process_fn=None if model is None else model._process_fn,
            predict_dataset_config=predict_dataset_config,
            predict_transform_config=predict_transform_config,
            tokenizer_config=tokenizer_config_obj,
            debug=False,
            data_name=data_name,
            datapath=datapath,
            cls=args.cls,
        )
        run_logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº†")
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        callbacks_list: List[Callback] = []
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        checkpoint_config = task_config.get('checkpoint')
        if checkpoint_config:
            checkpoint_config['dirpath'] = os.path.join('experiments', task_config.experiment_name)
            callbacks_list.append(ModelCheckpoint(**checkpoint_config))
            run_logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ : {checkpoint_config['dirpath']}")
        
        # ãƒ©ã‚¤ã‚¿ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        active_writer = None
        writer_config_dict = task_config.get('writer')
        if writer_config_dict:
            output_dir = args.output_dir or writer_config_dict.get('output_dir', './outputs')
            save_name = args.output or writer_config_dict.get('save_name', 'output')
            
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            writer_params = {
                **writer_config_dict,
                'output_dir': output_dir,
                'save_name': save_name,
                'order_config': predict_transform_config.order_config if predict_transform_config else None
            }
            
            # ä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            for key in ['npz_dir', 'user_mode', 'output_name', 'add_num', 'repeat']:
                writer_params.pop(key, None)
            
            active_writer = get_writer(**writer_params)
            callbacks_list.append(active_writer)
            run_logger.info(f"ãƒ©ã‚¤ã‚¿ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ : {type(active_writer)}")
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        system = None
        system_cfg_path = task_config.components.get('system')
        if system_cfg_path:
            system_loaded_cfg = load_config_safely('system', system_cfg_path)
            system = get_system(
                **system_loaded_cfg,
                model=model,
                steps_per_epoch=1,
            )
            run_logger.info(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–: {type(system)}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
        resume_from_checkpoint = task_config.get('resume_from_checkpoint')
        if resume_from_checkpoint:
            resume_from_checkpoint = download(resume_from_checkpoint)
            run_logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {resume_from_checkpoint}")
        
        # äºˆæ¸¬å®Ÿè¡Œè¦ä»¶ã®æ¤œè¨¼
        if not validate_prediction_requirements(task_config, resume_from_checkpoint):
            exit(1)
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
        trainer_config_dict = task_config.get('trainer', {})
        trainer = L.Trainer(
            callbacks=callbacks_list,
            **trainer_config_dict,
        )
        run_logger.info("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–å®Œäº†")
        
        # äºˆæ¸¬å®Ÿè¡Œ
        run_logger.info("äºˆæ¸¬å®Ÿè¡Œé–‹å§‹...")
        predictions_output = trainer.predict(system, datamodule=data_module, ckpt_path=resume_from_checkpoint)
        run_logger.info("äºˆæ¸¬å®Ÿè¡Œå®Œäº†")
        
        if predictions_output:
            run_logger.info(f"äºˆæ¸¬çµæœ: {len(predictions_output)}ãƒãƒƒãƒ")
        else:
            run_logger.warning("äºˆæ¸¬çµæœãŒç©ºã§ã™")
        
        if active_writer:
            run_logger.info(f"ãƒ©ã‚¤ã‚¿ãƒ¼å‡ºåŠ›å®Œäº†: {active_writer.output_dir}")
        
        run_logger.info("ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå®Œäº†")
        
    except Exception as e:
        run_logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e}")
        raise
